# This is a Databricks asset bundle definition for DLTTestBundle.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
bundle:
  name: DLTTestBundle

  git:
    origin_url: https://github.com/aiacovella/dlt-test-bundle.git
    branch: main

include:
  - resources/*.yml

resources:

  volumes:
    demo_volume:
      catalog_name: "${bundle.environment}_demo_catalog"
      name: "${bundle.environment}_demo_volume"
      schema_name: "default"

  pipelines:
    # Demo DLT Pipeline
    demo_dlt_pipeline:
      name: "[${bundle.environment}] Demo DLT Pipeline"
      target: "default" # Schema reference
      catalog: "${bundle.environment}_demo_catalog"

      libraries:
        - file:
            path: ./stream_utils.py
        - notebook:
            path: ./del-north_cdc.py

      channel: preview
      configuration:
        "bundle.file_path": "${workspace.file_path}"

  jobs:
    test_dlt_job:
      name: "${bundle.environment}_DLT_job"

      # These appear to have to be valid emails within Databricks user accounts
      email_notifications:
        on_start:
          - aiacovella@chariotsolutions.com
          - java.peritus@gmail.com

        on_success:
          - aiacovella@chariotsolutions.com
          - java.peritus@gmail.com

        on_failure:
          - aiacovella@chariotsolutions.com
          - java.peritus@gmail.com

      tasks:
        - task_key: dlt_test_pipeline
          pipeline_task:
            pipeline_id: ${resources.pipelines.demo_dlt_pipeline.id}
          libraries:
            - pypi:
                package: wheel==0.41.2
            - pypi:
                package: numpy==1.25.2
                repo: https://pypi.org/simple/

      schedule:
          quartz_cron_expression: "0 0 * * * ?"
          timezone_id: "UTC"
          pause_status: "UNPAUSED"


      # Start a new cluster with each job run
      job_clusters:
        - job_cluster_key: cluster
          new_cluster:
            spark_version: 15.4.x-scala2.12
            node_type_id: m5d.large
            num_workers: 2
            spark_conf:
              "spark.databricks.cluster.profile": "singleNode"
              "spark.master": "local[*, 4]"
            custom_tags:
              "ResourceClass": "SingleNode"

      # To user an existing cluster (not recommended since it requires an expensive general purpose cluster)
      #      job_cluster:
      #        - job_cluster_key: cluster
      #          existing_cluster_id: '0102-181314-mqlpg55p'

environments:
  development:
    default: true
    workspace:
      host: https://dbc-ed001880-9366.cloud.databricks.com

    resources:
      pipelines:
        demo_dlt_pipeline:
          development: true
          permissions:
            - level: CAN_VIEW
              group_name: users
          clusters:
            - node_type_id: m5d.large
            - num_workers: 2


  qa: # Used for test runs from a pull request
    workspace:
      host: https://dbc-ed001880-9366.cloud.databricks.com

    resources:
      pipelines:
        demo_dlt_pipeline:
          development: true
          permissions:
            - level: CAN_VIEW
              group_name: users
          clusters:
            - node_type_id: m5d.large
            - num_workers: 2

  production: # Used for test runs from a pull request
    workspace:
      host: https://dbc-ed001880-9366.cloud.databricks.com

    resources:
      pipelines:
        demo_dlt_pipeline:
          development: false
          photon: false
          permissions:
            - level: CAN_VIEW
              group_name: users
          clusters:
            - node_type_id: m5d.large
            - num_workers: 2
